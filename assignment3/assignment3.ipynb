{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 [48 marks, 15%]\n",
    "Hi all! Welcome to the 3rd assignment of this course! Here, you will learn how to build a singing voice transcription system (SVT) for songs, a system that help you convert singing voice in audio into note-level notations. This assignment contains 3 sections:\n",
    "1. Building necessary components for the system,\n",
    "2. The entire transcription workflow,\n",
    "3. Some extension questions.\n",
    "\n",
    "You are required to:\n",
    "- Finish this notebook. Successfully run all the code cells and answer all the questions.\n",
    "- When you need to embed screenshot in the notebook, put the picture in './resources'.\n",
    "- After finishing, **zip the whole directory (please exclude data_mini directory)**, then submit to Canvas. **Naming: \"eXXXXXXX_Name_Assignment3.zip\"**.\n",
    "\n",
    "This assignment constitutes 15% of your final grade, but **the full marks of this notebook are 48**. We will normalize your score when computing final grade, i.e., Your assignment 3 score = [Your Score] / 48 * 15.\n",
    "\n",
    "**Honor Code**\n",
    "Note that plagiarism will not be condoned. You may discuss the questions with your classmates or search on the internet for references, but you MUST NOT submit your code/answers that is copied directly from other sources. If you referred to the code or tutorial somewhere, please explicitly attribute the source somewhere in your code, e.g., in the comment.\n",
    "\n",
    "**Note**\n",
    "Please restart the jupyter kernel every time after modifying any py files. Otherwise, the updated code may not be loaded by this notebook.\n",
    "Restarting kernel won't clear the output of previous cells, your running history is still preserved.\n",
    "\n",
    "**Useful Resources**\n",
    "- [Music Transcription Overview](https://www.eecs.qmul.ac.uk/~simond/pub/2018/BenetosDixonDuanEwert-SPM2018-Transcription.pdf)\n",
    "- [Evaluation for Singing Transcription](https://riuma.uma.es/xmlui/bitstream/handle/10630/8372/298_Paper.pdf?sequence=1)\n",
    "- [mir_eval documentation](https://craffel.github.io/mir_eval/#mir_eval.transcription.evaluate)\n",
    "- [VOCANO: A note transcription framework for singing voice in polyphonic music](https://archives.ismir.net/ismir2021/paper/000036.pdf)\n",
    "- [TONet: Tone-Octave Network for Singing Melody Extraction from Polyphonic Music](http://arxiv.org/abs/2202.00951)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting started\n",
    "\n",
    "We recommend you to use a Conda environment for the course. If you have not yet done so, please\n",
    "1. Download [miniconda](https://docs.conda.io/en/latest/miniconda.html), install it on your computer. After installation, you may need to restart the command line (powershell/zsh/bash).\n",
    "2. When you see this (base) in your command line, it means a successful installation.\n",
    "               ![figure](./resources/conda.png)\n",
    "3. Create an environment called \"4347\", and then enter the environment:\n",
    "\n",
    "        conda create -n 4347 python=3.9\n",
    "        conda activate 4347\n",
    "4. Install packages\n",
    "\n",
    "        # Install PyTorch (use command suitable for your OS)\n",
    "        # Linux / Windows (CUDA 11.7)\n",
    "        pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117\n",
    "        # Windows (CPU)\n",
    "        pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu\n",
    "        # OSX (CPU)\n",
    "        pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1\n",
    "\n",
    "        # Install other libraries\n",
    "        pip install -r requirement.txt\n",
    "\n",
    "        # Install ffmpeg\n",
    "        conda install ffmpeg\n",
    "5. When you run this notebook in your IDE, switch the interpreter to the 4347 conda environment.\n",
    "6. You may be prompted to install the jupyter package. Click \"confirm\" in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 1 - Important Components [21 marks]\n",
    "We are going to use pytorch to build a neural network for SVT. Like all the deep learning projects, we start with data pipeline.\n",
    "\n",
    "### Task 1: Data Loader [3 marks]\n",
    "\n",
    "**YOUR TASKS:** Finish the code for MyDataset class in **dataset.py**, so that you can pass the test in the cell below. **[3 mark(s)]**\n",
    "\n",
    "You need to fill the code that implement:\n",
    "1. Convert note-level annotation to frame-level annotation.\n",
    "2. Read audio file, convert to [mel spectrogram](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). Some tools may help you, like [torchaudio](https://pytorch.org/audio/main/generated/torchaudio.transforms.MelSpectrogram.html) or [librosa](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html).\n",
    "3. Extract 5-s segments from spectrogram and annotations as samples for training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true,
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for data in tqdm(train_loader):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D],\n",
    "                                # i.e., [Batch size, num of frame per sample, spectrogram feature dimension]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    break\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Speed it up  [7 marks]\n",
    "\n",
    "You might have a working dataloader now, but it may have a drawback. Let's load 5 batches of data from your data loader. (Please run the code below) **[1 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D], i.e., [Batch size, num of frame per sample, feature dimention]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    if i == 4:\n",
    "        dur = time.time()-t0\n",
    "        est_time = dur / 5 * 123\n",
    "        break\n",
    "print('5 batches use {:.2f} seconds'.format(dur))\n",
    "print('Estimated time to load the whole training set: {:.2f} seconds.'.format(est_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the estimated time to load the whole training set is within 30s, congratulations!\n",
    "If you did not modify the workflow of the current dataset, it may take more than 1000 seconds to load the whole training set. Consequently, data loading become the bottleneck of time overhead.\n",
    "\n",
    "**YOUR TASK**:\n",
    "1. Answer the two questions below **[2 mark(s)]**\n",
    "2. Do necessary change to your dataset class. **[3 mark(s)]**  \n",
    "   ***Hint***:\n",
    "   - Each iteration within the dataset is supposed to return just a 5-s clip of audio, but it read the entire audio file into the RAM. Possibly when next time when we need to return another clip of the same audio file, the I/O operation can be possibly omitted.\n",
    "   - Same thing for annotation computation. Currently frame-level annotation is calculated once every time when we just need a 5-s clip.\n",
    "   - Another option is that is there are some ways to read just the required clip of audio from the hard disk instead of the entire song. You may need to do some research here. You are allowed to modify the way/format by which the audio are saved to the disk.\n",
    "3. Restart the kernel, and run the code cell below. **[1 mark(s)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Question:\n",
    "\n",
    "[1] What do you think causes the time overhead issue in the current data loading?  \n",
    "**[Answer: The time overhead in the current data loading process is mainly caused by:\n",
    "\n",
    "- Loading the Entire Audio File: Each time a 5-second clip is requested, the entire audio file is loaded into memory. \n",
    "This is inefficient, especially for long audio files, as only a small portion of the data is needed for the specific clip.\n",
    "- Computing Frame-Level Annotations Repeatedly: Frame-level annotations are recalculated for the entire song \n",
    "every time a 5-second clip is requested. This redundant computation adds significant time overhead\n",
    "because it is repeated unnecessarily for each clip.]**\n",
    "\n",
    "[2] What is your plan to modify your dataset class? State 2 possible solutions, and mention which do you choose to implement, together with the reason.  \n",
    "**[Two possible solutions to modify the MyDataset class and reduce time overhead are:\n",
    "\n",
    "1. Lazy Loading and Caching of Audio Clips:\n",
    "        Instead of loading the entire audio file each time, only the required 5-second clip should be loaded directly from the disk using a method that allows partial reading od files (like torchaudio's load with specified offsets and durations).\n",
    "        Once a clip is loaded, it can be cached in memory if needed again within the same session. This avoids redundant I/O operations for the same clip.\n",
    "\n",
    "2. Precompute and Cache Frame-Level Annotations:\n",
    "        Precompute frame-level annotations for the entire song once and cache them. When a 5-second clip is requested, slice the precomputed annotations to get the relevant segment.\n",
    "        This eliminates the redundant computation of annotations for each 5-second clip.\n",
    "\n",
    "Chosen Implementation:\n",
    "    I would choose to implement both solutions.\n",
    "    By optimizing the loading process to load only the required audio clip and caching the precomputed frame-level annotations,\n",
    "    we can significantly reduce the time overhead caused by both I/O operations and repeated calculations]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "import time\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D], i.e., [Batch size, num of frame per sample, feature dimention]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    if i == 4:\n",
    "        dur = time.time()-t0\n",
    "        est_time = dur / 5 * 123\n",
    "        break\n",
    "print('5 batches use {:.2f} seconds'.format(dur))\n",
    "print('Estimated time to load the whole training set: {:.2f} seconds.'.format(est_time))\n",
    "if est_time < 40:\n",
    "    print('Well Done!')\n",
    "else:\n",
    "    print('We can still be faster.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Loss function [3 marks]\n",
    "\n",
    "We are going to use \"multitask learning\" to train our model. I.e., we are training our model simultaneously on 4 tasks: 4 types of frame classification tasks. They are:\n",
    "- Classify if there is an onset on some frame.\n",
    "- Classify if there is an offset on some frame.\n",
    "- Classify the pitch of some frame lies on which octave (we have 4 octaves numbered 1~4, and 0 means silence).\n",
    "- Classify which pitch class if the current pitch (from C~B, 12 semitones, and 0 which means silence).\n",
    "\n",
    "In this case, the loss function is not that straightforward. To improve the readability of our code, we are going wrap the loss computation into a class.\n",
    "\n",
    "**YOUR TASKS:** Finish the code of train.LossFunc class, and run the cell below.  **[3 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from train import LossFunc\n",
    "\n",
    "loss_func = LossFunc(device='cpu')\n",
    "\n",
    "on_out = torch.rand(size=(8, 250))          # [B, T]\n",
    "off_out = torch.rand(size=(8, 250))\n",
    "octave_out = torch.rand(size=(8, 250, 5))   # [B, T, #Class]\n",
    "pitch_class_out = torch.rand(size=(8, 250, 13))\n",
    "\n",
    "on_tgt = on_out\n",
    "off_tgt = off_out\n",
    "octave_tgt = torch.randint(high=5, size=(8, 250))\n",
    "pitch_class_tgt = torch.randint(high=13, size=(8, 250))\n",
    "\n",
    "\n",
    "losses = loss_func.get_loss(\n",
    "    out=(on_out, off_out, octave_out, pitch_class_out),\n",
    "    tgt=(on_tgt, off_tgt, octave_tgt, pitch_class_tgt)\n",
    ")\n",
    "\n",
    "assert losses != None\n",
    "assert len(losses) == 5\n",
    "assert isinstance(losses[0], torch.Tensor)\n",
    "print('Succeed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Metric [3 marks]\n",
    "\n",
    "We need to observe the model performance during training to see how the training is going on. In addition to loss, we may also want to know the f1 score or accuracy, in both training loop and validation loop.\n",
    "To facilitate this, we wrap the metric computation to a single class.\n",
    "\n",
    "**YOUR TASKS:** Finish the code of train.Metric class, and run the cell below.  **[3 mark(s)]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "from train import LossFunc, Metrics\n",
    "\n",
    "loss_func = LossFunc(device='cpu')\n",
    "metric = Metrics(loss_func=loss_func)\n",
    "\n",
    "# dummy output\n",
    "on_out = torch.rand(size=(8, 250))          # [B, T]\n",
    "off_out = torch.rand(size=(8, 250))\n",
    "octave_out = torch.rand(size=(8, 250, 5))   # [B, T, #Class]\n",
    "pitch_class_out = torch.rand(size=(8, 250, 13))\n",
    "out = (on_out, off_out, octave_out, pitch_class_out)\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    tgt = (onset, offset, octave, pitch_class)\n",
    "\n",
    "    metric.update(out, tgt)\n",
    "    if i == 4:\n",
    "        break\n",
    "train_metric = metric.get_value()\n",
    "print(train_metric, '\\n')\n",
    "assert len(train_metric) == 9\n",
    "for k in train_metric:\n",
    "    assert train_metric[k] > 0\n",
    "assert metric.buffer == {}\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5: Model [5 marks]\n",
    "\n",
    "**YOUR TASKS:**\n",
    "1. Implement the model in model.BaseCNN_mini, following the description below **[4 mark(s)]**\n",
    "2. Successfully run the cell below. **[1 mark(s)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Model Description**\n",
    "1. This is a convolutional neural network that operates on spectrogram of a 5s-segment audio.\n",
    "2. Three 2-d convolutional layers at the beginning, each with 3x3 kernal size and 1x1 padding. Output channel number: 16, 32, 64.\n",
    "3. There is a batch normalization after each conv layer, then followed by ReLU as activation.\n",
    "4. Before the 2nd and 3rd conv layer, there are max pooling (kernel size=(1,2)) along the feature dimension. NOTE: do not shrink the time dimension, because we need to make prediction for each frame.\n",
    "5. After all convolution operation, permute the \"feature\" and \"channel\" dimensions so that they are adjacent, then merge the two dimensions to form a new feature dimension.\n",
    "6. There is a position-wise feed-forward layer with 256 dimention, i.e., for all frames along the time axis, convert all features from each frame into a 256-d vector. There is a ReLU activation function afterwards.\n",
    "7. Prediction heads for onset, offset, octave, pitch class, each of them is a linear layer. They receive output from feed-forward layer, and produce the final output. Note: no activation function for these last layers, e.g., sigmoid/softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import BaseCNN_mini\n",
    "\n",
    "model = BaseCNN_mini(feat_dim=256)\n",
    "dummy_input = torch.rand(size=(8, 250, 256))\n",
    "out = model(dummy_input)\n",
    "on, off, oct, pit = out\n",
    "\n",
    "assert list(on.shape) == [8, 250]\n",
    "assert list(off.shape) == [8, 250]\n",
    "assert list(oct.shape) == [8, 250, 5]\n",
    "assert list(pit.shape) == [8, 250, 13]\n",
    "\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 2 - Training and Evaluation [14 marks]\n",
    "\n",
    "### Task 6: Training [5 marks]\n",
    "Now we are ready for training! Both the model and data are not large, training can be performed in your laptop.\n",
    "Start training with the following command (estimated training time: 10 min):\n",
    "\n",
    "        python train.py\n",
    "You may need some time for debugging to successfully finish the training.\n",
    "\n",
    "### Task 7: Testing [3 marks]\n",
    "Then is testing. Test the model by\n",
    "\n",
    "        python test.py\n",
    "\n",
    "You may need to adjust the threshold values to make precision and recall to a similar value, to maximize the F1 score.\n",
    "\n",
    "The estimated performance: 48%, 38%, 17%, for COn, COnP, COnPOff, respectively.\n",
    "\n",
    "After finishing training or testing, please attach a screenshot that indicate training/testing is finished. **Please include your command line prompt in the screenshot to show that is you**.\n",
    "\n",
    "### Task 8: Visualization (Case Study) [6 marks]\n",
    "Finally, to have an intuitive understanding of the model's performance, please visualize the output of your model and the ground truth (annotation), for one segment of audio. You may choose whichever way you would like. And after that, please also attach your visualization figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[TODO: Please attach your screenshots in this block]\n",
    "\n",
    "**[Screenshot of finish of training]**\n",
    "\n",
    "**[Screenshot of finish of testing]**\n",
    "\n",
    "**[Visualization figure of output and ground truth]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 3 - Questions [13 marks]\n",
    "**YOUR TASKS:** Please answer the questions below:\n",
    "\n",
    "- How does the post-processing algorithm operates? How does it convert frame-level output to note-level output before the final evaluation? Briefly explain your understanding. (You may need to find the corresponding code from some py file and try to understand it before answering.) **[2 marks]**\n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- How did we compute the final performance metric for note-level transcription? Briefly explain how the computation is conducted for a pair of note-level output and annotation for one song. (Hint: look into the code first, and some \"useful resources\" attached at the beginning of this notebook may help.) **[2 marks]**\n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- Recall that we are using f1 score in the Metrics class to observe the onset/offset classification performance. Do you think such f1 score is a good performance in this case? What about using accuracy (correct prediction / num of frames) instead, do you think it's proper for onset/offset classification? If not, what are the reasons respectively for the two metrics? What are possible better choices?  **[2 marks]**\n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- How does your system performed? Briefly introduce your system's transcription performance (NOT time efficiency) with objective metric scores and your visualization.\n",
    "[Your answer]  **[2 marks]**\n",
    "</br>\n",
    "\n",
    "- The current system might not be performing very well on this dataset, or perform decently but still have room to improve. What are possible reasons for the not-so-good performance, and directions of improvement? Please list 3 pairs of them.\n",
    "[Your answer]  **[3 marks]**\n",
    "</br>\n",
    "\n",
    "- What do you think is the most difficult part? Which part did you spent most time on it? **[1 mark(s)]**\n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- How much time did you spent on the assignment? Please fill an estimated time here if you did not time yourself. **[1 mark(s)]**\n",
    "[Your answer]\n",
    "</br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
