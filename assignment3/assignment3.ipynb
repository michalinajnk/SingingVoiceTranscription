{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 3 [48 marks, 15%]\n",
    "Hi all! Welcome to the 3rd assignment of this course! Here, you will learn how to build a singing voice transcription system (SVT) for songs, a system that help you convert singing voice in audio into note-level notations. This assignment contains 3 sections:\n",
    "1. Building necessary components for the system,\n",
    "2. The entire transcription workflow,\n",
    "3. Some extension questions.\n",
    "\n",
    "You are required to:\n",
    "- Finish this notebook. Successfully run all the code cells and answer all the questions.\n",
    "- When you need to embed screenshot in the notebook, put the picture in './resources'.\n",
    "- After finishing, **zip the whole directory (please exclude data_mini directory)**, then submit to Canvas. **Naming: \"eXXXXXXX_Name_Assignment3.zip\"**.\n",
    "\n",
    "This assignment constitutes 15% of your final grade, but **the full marks of this notebook are 48**. We will normalize your score when computing final grade, i.e., Your assignment 3 score = [Your Score] / 48 * 15.\n",
    "\n",
    "**Honor Code**\n",
    "Note that plagiarism will not be condoned. You may discuss the questions with your classmates or search on the internet for references, but you MUST NOT submit your code/answers that is copied directly from other sources. If you referred to the code or tutorial somewhere, please explicitly attribute the source somewhere in your code, e.g., in the comment.\n",
    "\n",
    "**Note**\n",
    "Please restart the jupyter kernel every time after modifying any py files. Otherwise, the updated code may not be loaded by this notebook.\n",
    "Restarting kernel won't clear the output of previous cells, your running history is still preserved.\n",
    "\n",
    "**Useful Resources**\n",
    "- [Music Transcription Overview](https://www.eecs.qmul.ac.uk/~simond/pub/2018/BenetosDixonDuanEwert-SPM2018-Transcription.pdf)\n",
    "- [Evaluation for Singing Transcription](https://riuma.uma.es/xmlui/bitstream/handle/10630/8372/298_Paper.pdf?sequence=1)\n",
    "- [mir_eval documentation](https://craffel.github.io/mir_eval/#mir_eval.transcription.evaluate)\n",
    "- [VOCANO: A note transcription framework for singing voice in polyphonic music](https://archives.ismir.net/ismir2021/paper/000036.pdf)\n",
    "- [TONet: Tone-Octave Network for Singing Melody Extraction from Polyphonic Music](http://arxiv.org/abs/2202.00951)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting started\n",
    "\n",
    "We recommend you to use a Conda environment for the course. If you have not yet done so, please\n",
    "1. Download [miniconda](https://docs.conda.io/en/latest/miniconda.html), install it on your computer. After installation, you may need to restart the command line (powershell/zsh/bash).\n",
    "2. When you see this (base) in your command line, it means a successful installation.\n",
    "               ![figure](./resources/conda.png)\n",
    "3. Create an environment called \"4347\", and then enter the environment:\n",
    "\n",
    "        conda create -n 4347 python=3.9\n",
    "        conda activate 4347\n",
    "4. Install packages\n",
    "\n",
    "        # Install PyTorch (use command suitable for your OS)\n",
    "        # Linux / Windows (CUDA 11.7)\n",
    "        pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117\n",
    "        # Windows (CPU)\n",
    "        pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu\n",
    "        # OSX (CPU)\n",
    "        pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1\n",
    "\n",
    "        # Install other libraries\n",
    "        pip install -r requirement.txt\n",
    "\n",
    "        # Install ffmpeg\n",
    "        conda install ffmpeg\n",
    "5. When you run this notebook in your IDE, switch the interpreter to the 4347 conda environment.\n",
    "6. You may be prompted to install the jupyter package. Click \"confirm\" in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 1 - Important Components [21 marks]\n",
    "We are going to use pytorch to build a neural network for SVT. Like all the deep learning projects, we start with data pipeline.\n",
    "\n",
    "### Task 1: Data Loader [3 marks]\n",
    "\n",
    "**YOUR TASKS:** Finish the code for MyDataset class in **dataset.py**, so that you can pass the test in the cell below. **[3 mark(s)]**\n",
    "\n",
    "You need to fill the code that implement:\n",
    "1. Convert note-level annotation to frame-level annotation.\n",
    "2. Read audio file, convert to [mel spectrogram](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). Some tools may help you, like [torchaudio](https://pytorch.org/audio/main/generated/torchaudio.transforms.MelSpectrogram.html) or [librosa](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html).\n",
    "3. Extract 5-s segments from spectrogram and annotations as samples for training.\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T03:24:21.254804Z",
     "start_time": "2024-09-14T03:24:21.243822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "    print(\"Set default tensor type to 'torch.cuda.FloatTensor'.\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n",
      "Set default tensor type to 'torch.cuda.FloatTensor'.\n",
      "PyTorch Version: 2.0.0+cu117\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T04:24:54.646117Z",
     "start_time": "2024-09-14T04:24:53.392147Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader\n",
    "from hparams import Hparams\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256'\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "torch.cuda.empty_cache()\n",
    "try:\n",
    "    for data in tqdm(train_loader):\n",
    "        x, onset, offset, octave, pitch_class = data  # Data already on the correct device\n",
    "        assert list(x.shape) == [8, 250, 256]  \n",
    "        assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "        # Clear the cache after processing each batch\n",
    "        torch.cuda.empty_cache()\n",
    "    print('Congrats!')\n",
    "except RuntimeError as e:\n",
    "    print(f\"A runtime error occurred: {str(e)}\")\n",
    "    # Manually clear CUDA cache\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 354.00 MiB (GPU 0; 2.00 GiB total capacity; 1.01 GiB already allocated; 150.43 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m      6\u001B[0m os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPYTORCH_CUDA_ALLOC_CONF\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmax_split_size_mb:256\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m----> 7\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m \u001B[43mget_data_loader\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mHparams\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32m~\\assignment3\\assignment3\\dataset.py:42\u001B[0m, in \u001B[0;36mget_data_loader\u001B[1;34m(split, args, fns)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_data_loader\u001B[39m(split, args, fns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;124;03m    Create a DataLoader for the dataset with optimized settings.\u001B[39;00m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 42\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[43mMyDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset_root\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdataset_root\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     44\u001B[0m \u001B[43m        \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msplit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[43m        \u001B[49m\u001B[43msampling_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msampling_rate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m        \u001B[49m\u001B[43mannotation_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mannotation_path\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msample_length\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m        \u001B[49m\u001B[43mframe_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mframe_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     49\u001B[0m \u001B[43m        \u001B[49m\u001B[43msong_fns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     50\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcuda\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\n\u001B[0;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m     \u001B[38;5;66;03m# Optimized DataLoader configuration\u001B[39;00m\n\u001B[0;32m     54\u001B[0m     data_loader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[0;32m     55\u001B[0m         dataset,\n\u001B[0;32m     56\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m'\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     62\u001B[0m         pin_memory_device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m     63\u001B[0m     )\n",
      "File \u001B[1;32m~\\assignment3\\assignment3\\dataset.py:113\u001B[0m, in \u001B[0;36mMyDataset.__init__\u001B[1;34m(self, dataset_root, split, sampling_rate, annotation_path, sample_length, frame_size, device, song_fns)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindex_data(sample_length)\n\u001B[0;32m    112\u001B[0m \u001B[38;5;66;03m# Preload and preprocess all data\u001B[39;00m\n\u001B[1;32m--> 113\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpreload_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\assignment3\\assignment3\\dataset.py:122\u001B[0m, in \u001B[0;36mMyDataset.preload_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maudio_cache[fn] \u001B[38;5;241m=\u001B[39m full_audio\n\u001B[0;32m    121\u001B[0m annotations \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_and_cache_annotations(fn)\n\u001B[1;32m--> 122\u001B[0m mel_spectrogram \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_mel_spectrogram\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfull_audio\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msampling_rate\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\assignment3\\assignment3\\dataset.py:195\u001B[0m, in \u001B[0;36mMyDataset.get_mel_spectrogram\u001B[1;34m(self, audio, fn, sample_rate)\u001B[0m\n\u001B[0;32m    185\u001B[0m mel_spectrogram_transform \u001B[38;5;241m=\u001B[39m torchaudio\u001B[38;5;241m.\u001B[39mtransforms\u001B[38;5;241m.\u001B[39mMelSpectrogram(\n\u001B[0;32m    186\u001B[0m     sample_rate\u001B[38;5;241m=\u001B[39msample_rate,\n\u001B[0;32m    187\u001B[0m     n_fft\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2048\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    191\u001B[0m     f_max\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8000\u001B[39m\n\u001B[0;32m    192\u001B[0m )\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)  \u001B[38;5;66;03m# Ensure the transform is on the correct device\u001B[39;00m\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# Apply the transform\u001B[39;00m\n\u001B[1;32m--> 195\u001B[0m mel_spectrogram \u001B[38;5;241m=\u001B[39m \u001B[43mmel_spectrogram_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43maudio\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    196\u001B[0m mel_spectrogram \u001B[38;5;241m=\u001B[39m mel_spectrogram\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m)  \u001B[38;5;66;03m# Adjust dimensions as needed\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmel_cache[fn] \u001B[38;5;241m=\u001B[39m mel_spectrogram\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\4347\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\4347\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:650\u001B[0m, in \u001B[0;36mMelSpectrogram.forward\u001B[1;34m(self, waveform)\u001B[0m\n\u001B[0;32m    642\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, waveform: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    643\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    644\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    645\u001B[0m \u001B[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    648\u001B[0m \u001B[38;5;124;03m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001B[39;00m\n\u001B[0;32m    649\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 650\u001B[0m     specgram \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspectrogram\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwaveform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    651\u001B[0m     mel_specgram \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmel_scale(specgram)\n\u001B[0;32m    652\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mel_specgram\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\4347\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\4347\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:110\u001B[0m, in \u001B[0;36mSpectrogram.forward\u001B[1;34m(self, waveform)\u001B[0m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, waveform: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    101\u001B[0m     \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;124;03m        Fourier bins, and time is the number of window hops (n_frame).\u001B[39;00m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspectrogram\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwaveform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_fft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwin_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpower\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormalized\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcenter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monesided\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\4347\\lib\\site-packages\\torchaudio\\functional\\functional.py:126\u001B[0m, in \u001B[0;36mspectrogram\u001B[1;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001B[0m\n\u001B[0;32m    123\u001B[0m waveform \u001B[38;5;241m=\u001B[39m waveform\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, shape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m    125\u001B[0m \u001B[38;5;66;03m# default values are consistent with librosa.core.spectrum._spectrogram\u001B[39;00m\n\u001B[1;32m--> 126\u001B[0m spec_f \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstft\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwaveform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_fft\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_fft\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    130\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwin_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwin_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    131\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwindow\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    132\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcenter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcenter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    133\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpad_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    134\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnormalized\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mframe_length_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43monesided\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monesided\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_complex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    137\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;66;03m# unpack batch\u001B[39;00m\n\u001B[0;32m    140\u001B[0m spec_f \u001B[38;5;241m=\u001B[39m spec_f\u001B[38;5;241m.\u001B[39mreshape(shape[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m spec_f\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m:])\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\4347\\lib\\site-packages\\torch\\functional.py:641\u001B[0m, in \u001B[0;36mstft\u001B[1;34m(input, n_fft, hop_length, win_length, window, center, pad_mode, normalized, onesided, return_complex)\u001B[0m\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mview(extended_shape), [pad, pad], pad_mode)\n\u001B[0;32m    640\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39msignal_dim:])\n\u001B[1;32m--> 641\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstft\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_fft\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhop_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwin_length\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwindow\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[attr-defined]\u001B[39;49;00m\n\u001B[0;32m    642\u001B[0m \u001B[43m                \u001B[49m\u001B[43mnormalized\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43monesided\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_complex\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 354.00 MiB (GPU 0; 2.00 GiB total capacity; 1.01 GiB already allocated; 150.43 MiB free; 1.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Speed it up  [7 marks]\n",
    "\n",
    "You might have a working dataloader now, but it may have a drawback. Let's load 5 batches of data from your data loader. (Please run the code below) **[1 mark(s)]**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def convert_and_remove_mp3(source_folder):\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".mp3\"):  # Case insensitive matching\n",
    "                # Construct the full file path for the source mp3\n",
    "                full_path_mp3 = os.path.join(root, file)\n",
    "                # Define the target path for the wav file in the same folder\n",
    "                full_path_wav = os.path.join(root, os.path.splitext(file)[0] + \".wav\")\n",
    "\n",
    "                # Command to convert mp3 to wav\n",
    "                cmd = ['ffmpeg', '-i', full_path_mp3, full_path_wav, '-y']\n",
    "                # Execute the command\n",
    "                result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "                # Check if conversion was successful before deleting the mp3\n",
    "                if result.returncode == 0:  # Success\n",
    "                    os.remove(full_path_mp3)\n",
    "                else:\n",
    "                    print(f\"Failed to convert {full_path_mp3}\")\n",
    "\n",
    "source_dir = 'C:/Users/Michalina/assignment3/assignment3/data_mini'\n",
    "convert_and_remove_mp3(source_dir)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-09-14T03:05:28.633335Z",
     "start_time": "2024-09-14T03:05:00.491115Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = data\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D], i.e., [Batch size, num of frame per sample, feature dimention]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    if i == 4:\n",
    "        dur = time.time()-t0\n",
    "        est_time = dur / 5 * 123\n",
    "        break\n",
    "print('5 batches use {:.2f} seconds'.format(dur))\n",
    "print('Estimated time to load the whole training set: {:.2f} seconds.'.format(est_time))"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 4/123 [00:27<13:52,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 batches use 28.12 seconds\n",
      "Estimated time to load the whole training set: 691.74 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If the estimated time to load the whole training set is within 30s, congratulations!\n",
    "If you did not modify the workflow of the current dataset, it may take more than 1000 seconds to load the whole training set. Consequently, data loading become the bottleneck of time overhead.\n",
    "\n",
    "**YOUR TASK**:\n",
    "1. Answer the two questions below **[2 mark(s)]**\n",
    "2. Do necessary change to your dataset class. **[3 mark(s)]**  \n",
    "   ***Hint***:\n",
    "   - Each iteration within the dataset is supposed to return just a 5-s clip of audio, but it read the entire audio file into the RAM. Possibly when next time when we need to return another clip of the same audio file, the I/O operation can be possibly omitted.\n",
    "   - Same thing for annotation computation. Currently frame-level annotation is calculated once every time when we just need a 5-s clip.\n",
    "   - Another option is that is there are some ways to read just the required clip of audio from the hard disk instead of the entire song. You may need to do some research here. You are allowed to modify the way/format by which the audio are saved to the disk.\n",
    "3. Restart the kernel, and run the code cell below. **[1 mark(s)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Question:\n",
    "\n",
    "[1] What do you think causes the time overhead issue in the current data loading?  \n",
    "    \n",
    "    [a] Repeated Disk I/O Operations: Each call to __getitem__ potentially involves reading a segment from a disk if the segment is not cached. This repeated disk I/O is expensive, especially for larger datasets and frequent access patterns where the same audio file might be accessed multiple times.\n",
    "    \n",
    "    [b] On-the-fly Audio Processing: Each segment undergoes processing such as Mel Spectrogram conversion during every single fetch. This processing, especially if not utilizing GPU acceleration, can be computationally expensive.\n",
    "    \n",
    "    [c] Lack of Full Utilization of Caching: While there is a mechanism to cache audio data and annotations, the strategy can be further optimized. Initially loading entire audio files regardless of the length of the segments needed multiple times can still cause unnecessary overhead if not every part of the audio is used in the training.\n",
    "\n",
    "    [d] Moving data from ONE DEVICE TO ANOTHER (cpu -> gpu). To enhance efficiency and reduce processing time when handling audio data in PyTorch, especially when working with GPUs, IT IS good to preprocess the audio data by moving all audio samples to the designated device in advance. This approach minimizes the overhead caused by moving data back and forth between the CPU and GPU during training or inference.\n",
    "    \n",
    "[2] What is your plan to modify your dataset class? State 2 possible solutions, and mention which do you choose to implement, together with the reason.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "import time\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D], i.e., [Batch size, num of frame per sample, feature dimention]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    if i == 4:\n",
    "        dur = time.time()-t0\n",
    "        est_time = dur / 5 * 123\n",
    "        break\n",
    "print('5 batches use {:.2f} seconds'.format(dur))\n",
    "print('Estimated time to load the whole training set: {:.2f} seconds.'.format(est_time))\n",
    "if est_time < 40:\n",
    "    print('Well Done!')\n",
    "else:\n",
    "    print('We can still be faster.')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataset import load_audio_segment, load_audio_segment_torchaudio\n",
    "import time \n",
    "\n",
    "# Benchmark parameters\n",
    "file_path = r'C:\\Users\\Michalina\\assignment3\\assignment3\\data_mini\\example\\1\\Mixture.wav'\n",
    "start_sec = 0\n",
    "duration_sec = 5\n",
    "sample_rate = 16000\n",
    "\n",
    "# Benchmark Soundfile\n",
    "start_time = time.time()\n",
    "audio_sf = load_audio_segment(file_path, start_sec, duration_sec, sample_rate)\n",
    "sf_time = time.time() - start_time\n",
    "\n",
    "# Benchmark Torchaudio\n",
    "start_time = time.time()\n",
    "audio_ta = load_audio_segment_torchaudio(file_path, start_sec, duration_sec, sample_rate)\n",
    "ta_time = time.time() - start_time\n",
    "\n",
    "print(f\"Soundfile load time: {sf_time:.4f} seconds\")\n",
    "print(f\"Torchaudio load time: {ta_time:.4f} seconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Loss function [3 marks]\n",
    "\n",
    "We are going to use \"multitask learning\" to train our model. I.e., we are training our model simultaneously on 4 tasks: 4 types of frame classification tasks. They are:\n",
    "- Classify if there is an onset on some frame.\n",
    "- Classify if there is an offset on some frame.\n",
    "- Classify the pitch of some frame lies on which octave (we have 4 octaves numbered 1~4, and 0 means silence).\n",
    "- Classify which pitch class if the current pitch (from C~B, 12 semitones, and 0 which means silence).\n",
    "\n",
    "In this case, the loss function is not that straightforward. To improve the readability of our code, we are going wrap the loss computation into a class.\n",
    "\n",
    "**YOUR TASKS:** Finish the code of train.LossFunc class, and run the cell below.  **[3 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "from train import LossFunc\n",
    "\n",
    "loss_func = LossFunc(device='cpu')\n",
    "\n",
    "on_out = torch.rand(size=(8, 250))          # [B, T]\n",
    "off_out = torch.rand(size=(8, 250))\n",
    "octave_out = torch.rand(size=(8, 250, 5))   # [B, T, #Class]\n",
    "pitch_class_out = torch.rand(size=(8, 250, 13))\n",
    "\n",
    "on_tgt = on_out\n",
    "off_tgt = off_out\n",
    "octave_tgt = torch.randint(high=5, size=(8, 250))\n",
    "pitch_class_tgt = torch.randint(high=13, size=(8, 250))\n",
    "\n",
    "\n",
    "losses = loss_func.get_loss(\n",
    "    out=(on_out, off_out, octave_out, pitch_class_out),\n",
    "    tgt=(on_tgt, off_tgt, octave_tgt, pitch_class_tgt)\n",
    ")\n",
    "\n",
    "assert losses != None\n",
    "assert len(losses) == 5\n",
    "assert isinstance(losses[0], torch.Tensor)\n",
    "print('Succeed!')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Metric [3 marks]\n",
    "\n",
    "We need to observe the model performance during training to see how the training is going on. In addition to loss, we may also want to know the f1 score or accuracy, in both training loop and validation loop.\n",
    "To facilitate this, we wrap the metric computation to a single class.\n",
    "\n",
    "**YOUR TASKS:** Finish the code of train.Metric class, and run the cell below.  **[3 mark(s)]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "from train import LossFunc, Metrics\n",
    "\n",
    "loss_func = LossFunc(device='cpu')\n",
    "metric = Metrics(loss_func=loss_func)\n",
    "\n",
    "# dummy output\n",
    "on_out = torch.rand(size=(8, 250))          # [B, T]\n",
    "off_out = torch.rand(size=(8, 250))\n",
    "octave_out = torch.rand(size=(8, 250, 5))   # [B, T, #Class]\n",
    "pitch_class_out = torch.rand(size=(8, 250, 13))\n",
    "out = (on_out, off_out, octave_out, pitch_class_out)\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    tgt = (onset, offset, octave, pitch_class)\n",
    "\n",
    "    metric.update(out, tgt)\n",
    "    if i == 4:\n",
    "        break\n",
    "train_metric = metric.get_value()\n",
    "print(train_metric, '\\n')\n",
    "assert len(train_metric) == 9\n",
    "for k in train_metric:\n",
    "    assert train_metric[k] > 0\n",
    "assert metric.buffer == {}\n",
    "print('Congrats!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5: Model [5 marks]\n",
    "\n",
    "**YOUR TASKS:**\n",
    "1. Implement the model in model.BaseCNN_mini, following the description below **[4 mark(s)]**\n",
    "2. Successfully run the cell below. **[1 mark(s)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Model Description**\n",
    "1. This is a convolutional neural network that operates on spectrogram of a 5s-segment audio.\n",
    "2. Three 2-d convolutional layers at the beginning, each with 3x3 kernal size and 1x1 padding. Output channel number: 16, 32, 64.\n",
    "3. There is a batch normalization after each conv layer, then followed by ReLU as activation.\n",
    "4. Before the 2nd and 3rd conv layer, there are max pooling (kernel size=(1,2)) along the feature dimension. NOTE: do not shrink the time dimension, because we need to make prediction for each frame.\n",
    "5. After all convolution operation, permute the \"feature\" and \"channel\" dimensions so that they are adjacent, then merge the two dimensions to form a new feature dimension.\n",
    "6. There is a position-wise feed-forward layer with 256 dimention, i.e., for all frames along the time axis, convert all features from each frame into a 256-d vector. There is a ReLU activation function afterwards.\n",
    "7. Prediction heads for onset, offset, octave, pitch class, each of them is a linear layer. They receive output from feed-forward layer, and produce the final output. Note: no activation function for these last layers, e.g., sigmoid/softmax."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "from model import BaseCNN_mini\n",
    "model = BaseCNN_mini(feat_dim=256)\n",
    "dummy_input = torch.rand(size=(8, 250, 256))\n",
    "out = model(dummy_input)\n",
    "on, off, oct, pit = out\n",
    "\n",
    "assert list(on.shape) == [8, 250]\n",
    "assert list(off.shape) == [8, 250]\n",
    "assert list(oct.shape) == [8, 250, 5]\n",
    "assert list(pit.shape) == [8, 250, 13]\n",
    "\n",
    "print('Congrats!')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 2 - Training and Evaluation [14 marks]\n",
    "\n",
    "### Task 6: Training [5 marks]\n",
    "Now we are ready for training! Both the model and data are not large, training can be performed in your laptop.\n",
    "Start training with the following command (estimated training time: 10 min):\n",
    "\n",
    "        python train.py\n",
    "You may need some time for debugging to successfully finish the training.\n",
    "\n",
    "### Task 7: Testing [3 marks]\n",
    "Then is testing. Test the model by\n",
    "\n",
    "        python test.py\n",
    "\n",
    "You may need to adjust the threshold values to make precision and recall to a similar value, to maximize the F1 score.\n",
    "\n",
    "COnPOff (Complete Onset and Offset Precision) VERY LOW DUE TO DATA IMBALANCE!!!\n",
    "COnP (Complete Onset Precision) Al\n",
    "COn (Onset Precision):\n",
    "\n",
    "The estimated performance: 48%, 38%, 17%, for COn, COnP, COnPOff, respectively.\n",
    "\n",
    "After finishing training or testing, please attach a screenshot that indicate training/testing is finished. **Please include your command line prompt in the screenshot to show that is you**.\n",
    "\n",
    "### Task 8: Visualization (Case Study) [6 marks]\n",
    "Finally, to have an intuitive understanding of the model's performance, please visualize the output of your model and the ground truth (annotation), for one segment of audio. You may choose whichever way you would like. And after that, please also attach your visualization figure below."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from assignment3.train import AST_Model\n",
    "from visualize import Visualizer\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AST_Model(device, 'C:/Users/Michalina/assignment3/assignment3/results/best_model.pth')\n",
    "vis = Visualizer(model,train_loader, device)\n",
    "vis.visualize_results()\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[TODO: Please attach your screenshots in this block]\n",
    "**[Screenshot of finish of training]**\n",
    "**![alt text][C:/Users/Michalina/assignment3/results/finish_train.png]**\n",
    "\n",
    "\n",
    "**[Screenshot of finish of testing]**\n",
    "**![alt text][C:/Users/Michalina/assignment3/results/finish_test.png]**\n",
    "\n",
    "**[Visualization figure of output and ground truth]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 3 - Questions [13 marks]\n",
    "**YOUR TASKS:** Please answer the questions below:\n",
    "\n",
    "- How does the post-processing algorithm operates? How does it convert frame-level output to note-level output before the final evaluation? Briefly explain your understanding. (You may need to find the corresponding code from some py file and try to understand it before answering.) **[2 marks]**\n",
    "[\n",
    "The post-processing algorithm converts frame-level outputs from the model into note-level outputs \n",
    "by identifying contiguous segments of frames that correspond to the presence of musical notes. This is typically done by analyzing the onset and offset probabilities:\n",
    "\n",
    "1. Onset Detection: The algorithm first detects the frames where an onset (start of a note) is predicted. It identifies peaks in the onset probability above a certain threshold.\n",
    "2. Offset Detection: Similarly, it detects the frames where an offset (end of a note) is predicted.\n",
    "3. Octave and Pitch Classification: For each detected onset, the corresponding pitch class and octave are determined based on the frame-level predictions. The most frequent pitch and octave between an onset and its corresponding offset are chosen to represent the note.\n",
    "4. Note Formation: Once onsets and offsets are paired, the note information (start time, end time, pitch, and octave) is formed. This output is then used for evaluation against the ground truth.\n",
    "]\n",
    "</br>\n",
    "\n",
    "- How did we compute the final performance metric for note-level transcription? Briefly explain how the computation is conducted for a pair of note-level output and annotation for one song. (Hint: look into the code first, and some \"useful resources\" attached at the beginning of this notebook may help.) **[2 marks]**\n",
    "[\n",
    "Answer:\n",
    " - The final performance metric for note-level transcription is typically computed using precision, recall, and F1 score, comparing the predicted notes with the ground truth annotations:\n",
    "\n",
    "1. Matching Notes: Notes are matched between the predicted and ground truth notes based on onset time, offset time, and pitch. A predicted note is considered correct if it matches a ground truth note within a certain tolerance for onset/offset times and pitch.\n",
    "2. Precision and Recall: Precision is calculated as the ratio of correctly predicted notes to the total predicted notes, and recall is the ratio of correctly predicted notes to the total ground truth notes.\n",
    "3. F1 Score: The F1 score, which is the harmonic mean of precision and recall, provides a single metric that balances both aspects.\n",
    "]\n",
    "</br>\n",
    "\n",
    "- Recall that we are using f1 score in the Metrics class to observe the onset/offset classification performance. Do you think such f1 score is a good performance in this case? What about using accuracy (correct prediction / num of frames) instead, do you think it's proper for onset/offset classification? If not, what are the reasons respectively for the two metrics? What are possible better choices?  **[2 marks]**\n",
    "[\n",
    "Answer: \n",
    "- The F1 score is a good performance metric for onset/offset classification because it balances precision and recall, which is important when dealing with imbalanced datasets where the number of onsets/offsets (positive cases) is much smaller than the number of non-onset/offset frames (negative cases).\n",
    "\n",
    "Accuracy: Using accuracy as a metric in this scenario might not be proper because it could be misleading due to class imbalance. A model could predict \"no onset/offset\" for every frame and still achieve high accuracy because the majority of frames are indeed \"no onset/offset\".\n",
    "Better Choices: Other suitable metrics include precision-recall curves and the Area Under the Curve (AUC) for Precision-Recall, which better represent the model's performance on imbalanced data.\n",
    "]\n",
    "</br>\n",
    "\n",
    "- How does your system performed? Briefly introduce your system's transcription performance (NOT time efficiency) with objective metric scores and your visualization. #TODO\n",
    "[Your answer]  **[2 marks]**\n",
    "</br>\n",
    "\n",
    "- The current system might not be performing very well on this dataset, or perform decently but still have room to improve. What are possible reasons for the not-so-good performance, and directions of improvement? Please list 3 pairs of them.\n",
    "[\n",
    "Answer:\n",
    "Reason: Inadequate model complexity for capturing intricate musical patterns.\n",
    "\n",
    "1. Improvement: Increase model depth or use advanced architectures like Convolutional Recurrent Neural Networks (CRNN) or Transformers to better capture temporal dependencies.\n",
    "Reason: Insufficient data or poor data quality, leading to overfitting or underfitting.\n",
    "\n",
    "2. Improvement: Expand the dataset with more diverse samples or apply data augmentation techniques to create a more robust model.\n",
    "Reason: Misalignment between frame-level predictions and note-level requirements.\n",
    "\n",
    "3. Improvement: Implement more sophisticated post-processing algorithms that better convert frame-level predictions to note-level outputs, potentially using more advanced techniques like dynamic programming.\n",
    "]  **[3 marks]**\n",
    "</br>\n",
    "\n",
    "- What do you think is the most difficult part? Which part did you spent most time on it? **[1 mark(s)]**\n",
    "[Answer: \n",
    "- The most difficult part was designing and implementing the post-processing algorithm to convert frame-level predictions to note-level outputs. This requires understanding the nuances of music transcription and aligning predictions correctly with ground truth data. Most time was spent debugging and fine-tuning the model and post-processing code to ensure accurate predictions and evaluation.]\n",
    "</br>\n",
    "\n",
    "- How much time did you spent on the assignment? Please fill an estimated time here if you did not time yourself. **[1 mark(s)]**\n",
    "[Approximately 18-24 hours were spent on this assignment, including time for reading documentation, implementing code, debugging, and testing different parts of the system.]s\n",
    "</br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
